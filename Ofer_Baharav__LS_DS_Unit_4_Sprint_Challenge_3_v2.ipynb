{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ofer_Baharav_ LS_DS_Unit_4_Sprint_Challenge_3_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "u4-s3-dnn"
    },
    "kernelspec": {
      "display_name": "U4-S2-NNF-DS10",
      "language": "python",
      "name": "u4-s2-nnf-ds10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oferbaharav/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/Ofer_Baharav__LS_DS_Unit_4_Sprint_Challenge_3_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB6QLCRA7brT",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "# Major Neural Network Architectures Challenge\n",
        "## *Data Science Unit 4 Sprint 3 Challenge*\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science. This week we studied several famous neural network architectures: \n",
        "recurrent neural networks (RNNs), long short-term memory (LSTMs), convolutional neural networks (CNNs), and Autoencoders. In this sprint challenge, you will revisit these models. Remember, we are testing your knowledge of these architectures not your ability to fit a model with high accuracy. \n",
        "\n",
        "__*Caution:*__  these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime locally, on AWS SageMaker, on Colab or on a comparable environment. If something is running longer, double check your approach!\n",
        "\n",
        "## Challenge Objectives\n",
        "*You should be able to:*\n",
        "* <a href=\"#p1\">Part 1</a>: Train a LSTM classification model\n",
        "* <a href=\"#p2\">Part 2</a>: Utilize a pre-trained CNN for object detection\n",
        "* <a href=\"#p3\">Part 3</a>: Describe a use case for an autoencoder\n",
        "* <a href=\"#p4\">Part 4</a>: Describe yourself as a Data Science and elucidate your vision of AI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5UwGRnJOmD4"
      },
      "source": [
        "<a id=\"p1\"></a>\n",
        "## Part 1 - LSTMSs\n",
        "\n",
        "Use a LSTM to fit a multi-class classification model on Reuters news articles to distinguish topics of articles. The data is already encoded properly for use in a LSTM model. \n",
        "\n",
        "Your Tasks: \n",
        "- Use Keras to fit a predictive model, classifying news articles into topics. \n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well as the LSTM code we used in class.\n",
        "\n",
        "__*Note:*__  Focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DS-9ksWjoJit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8ad6f263-ada9-43ed-9a3a-c5872841a413"
      },
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None,\n",
        "                                                         skip_top=0,\n",
        "                                                         maxlen=None,\n",
        "                                                         test_split=0.2,\n",
        "                                                         seed=723812,\n",
        "                                                         start_char=1,\n",
        "                                                         oov_char=2,\n",
        "                                                         index_from=3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnswf635__qj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a2f6810-1dec-45bd-c59c-6e0ddb2d4854"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMDjEExTAEgo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66f482c1-1632-4c3e-f08e-97373255adcf"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8982,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzg8yN02AHsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b400d719-9333-4734-d6b9-d55a0ac99251"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2246,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqEtAseYANd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aae07286-a985-4d95-f389-8f2c37974ebd"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2246,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-FFc2I4AZxH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "bec9c2d4-2e9c-4d35-befb-6034bc711075"
      },
      "source": [
        "X_train[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 248, 409, 166, 265, 1537, 1662, 8, 24, 4, 1222, 2771, 7, 227, 236, 40, 85, 944, 10, 531, 176, 8, 4, 176, 1613, 24, 1662, 297, 5157, 6, 10, 103, 5, 231, 215, 8, 7, 2889, 6, 10, 1202, 69, 4, 1222, 329, 2771, 24, 944, 23, 944, 1662, 40, 2509, 1592, 907, 69, 4, 113, 997, 762, 2539, 7, 227, 236, 17, 12]),\n",
              "       list([1, 4665, 1183, 413, 381, 7, 1134, 1664, 62, 729, 7, 4, 121, 273, 93, 109, 28, 2115, 72, 11, 428, 4, 387, 989, 558, 3956, 8, 7, 25, 1213, 427, 1969, 223, 4, 213, 5, 387, 580, 8, 1145, 413, 62, 410, 451, 18, 428, 7, 4, 121, 6, 3106, 19, 11, 428, 9, 1283, 317, 65, 413, 138, 59, 12, 11, 428, 6, 6118, 63, 11, 4, 3956, 8, 3640, 1183, 413, 202, 251, 18, 428, 6, 546, 19, 11, 428, 9, 317, 65, 413, 7, 4, 1721, 427, 409, 7145, 138, 19, 19, 11, 428, 6, 3843, 70, 11, 4, 135, 5, 137, 317, 1833, 542, 9, 7145, 413, 138, 72, 47, 11, 428, 6, 19, 5106, 19, 16, 8, 17, 12]),\n",
              "       list([1, 56, 14065, 65, 9, 249, 149, 8, 4, 347, 5, 25, 65, 9, 249, 282, 333, 27, 258, 20, 6, 644, 59, 11, 15, 22, 653, 32, 11, 15, 257, 28, 29, 153, 105, 519, 6, 42, 1436, 7233, 14065, 8, 16, 40, 282, 5, 32, 47, 11, 428, 5, 65, 9, 659, 249, 3264, 9, 934, 32, 35, 1396, 983, 5, 659, 249, 7, 788, 388, 20, 324, 56, 26262, 705, 149, 40, 342, 282, 5, 2639, 18, 428, 5, 65, 9, 19, 59, 10637, 5, 659, 249, 31, 10, 143, 347, 5, 32, 32, 11, 15, 14065, 8, 17, 12]),\n",
              "       list([1, 346, 273, 94, 187, 53, 74, 472, 26, 14, 46, 19, 124, 15, 39, 74, 32, 6582, 18, 14, 46, 61, 6097, 18, 1730, 1668, 32, 11, 14, 996, 12, 11, 123, 346, 39, 235, 627, 276, 5, 19, 19, 11, 15, 17, 12]),\n",
              "       list([1, 1228, 81, 8, 261, 524, 10, 384, 292, 322, 5, 4, 49, 31, 10, 323, 5, 1400, 103, 450, 16, 8, 1400, 381, 69, 928, 20, 5, 1228, 80, 480, 10, 334, 116, 9, 100, 766, 157, 7, 4, 29, 31, 4, 1395, 5, 4, 587, 1400, 33, 2356, 6, 219, 767, 15, 10, 67, 6, 4, 1004, 1228, 261, 17, 12]),\n",
              "       list([1, 67, 416, 170, 54, 1223, 27, 356, 697, 81, 13161, 215, 8, 4, 116, 23, 3283, 8, 3522, 12484, 42, 559, 31, 1009, 716, 1578, 1838, 24, 16, 535, 45, 2746, 4, 834, 1802, 117, 4, 326, 2349, 347, 267, 21, 186, 828, 9668, 50, 40, 63, 61, 11, 79, 335, 34, 238, 28, 722, 19, 63, 233, 12, 63, 9668, 826, 195, 5, 313, 7135, 27569, 8, 36, 114, 45, 594, 21, 4, 9148, 5, 4, 116, 58, 21, 185, 4, 166, 41, 668, 6, 888, 16, 12484, 8, 24, 184, 9668, 34, 238, 28, 210, 492, 15, 10, 67, 28, 4, 216, 5, 4, 615, 416, 25, 1020, 24, 314, 289, 5, 266, 186, 1423, 1014, 25, 347, 9668, 8, 25, 390, 98, 186, 41, 1740, 215, 1423, 5, 296, 6, 124, 26, 10, 67, 28, 24, 96, 9668, 114, 1070, 10, 80, 96, 7, 4, 456, 22581, 12484, 8, 1578, 553, 25, 89, 186, 862, 13, 4, 2050, 362, 907, 6, 19, 279, 15, 10, 67, 22, 19, 119, 15, 21, 4, 17213, 36, 8, 16, 2351, 25, 448, 5692, 6, 19, 512, 15, 10, 67, 22, 19, 482, 15, 7, 788, 215, 8, 4, 7923, 2050, 362, 218, 1043, 922, 159, 917, 4, 182, 40, 3020, 22, 158, 6, 352, 20, 117, 4, 54, 142, 206, 267, 21, 186, 90, 67, 8, 5856, 19657, 42, 559, 31, 4296, 3367, 9, 111, 4, 182, 23, 133, 6, 351, 815, 28, 102, 6, 352, 20, 117, 4, 225, 142, 206, 36, 8, 12484, 8, 4, 688, 150, 10, 67, 116, 58, 2304, 11, 15, 135, 41, 30, 10, 7096, 13, 356, 697, 4, 1697, 9, 387, 49, 41, 286, 276, 4358, 172, 10, 1787, 218, 9, 3685, 419, 538, 1469, 36, 8, 356, 697, 40, 432, 55, 6709, 6, 30, 126, 2482, 15980, 30377, 692, 5, 25, 595, 36, 8, 28383, 21911, 4611, 195, 5, 29555, 65, 111, 24315, 50, 1043, 10, 124, 47, 20, 310, 7, 9668, 8, 54, 121, 356, 8568, 116, 34, 8197, 3283, 17, 12]),\n",
              "       list([1, 178, 53, 279, 26, 14, 124, 26, 178, 39, 19, 4481, 18, 14, 19, 4737, 18, 86, 187, 61, 11, 14, 160, 70, 11, 180, 183, 12, 4090, 5007, 14, 12, 2713, 628, 29, 178, 53, 462, 26, 14, 472, 26, 178, 39, 32, 4253, 18, 14, 19, 1890, 18, 86, 688, 70, 11, 14, 352, 61, 11, 180, 183, 12, 5219, 5350, 14, 12, 4400, 5306, 123, 29, 301, 1025, 1532, 627, 276, 5, 19, 19, 11, 15, 58, 156, 26, 53, 235, 629, 5, 5432, 18, 15, 14, 68, 11, 15, 7, 346, 9, 32, 32, 11, 15, 14, 19, 19, 11, 15, 7, 29, 22, 131, 46, 2555, 17, 12]),\n",
              "       list([1, 4, 30097, 106, 8, 16, 33, 8017, 4, 278, 324, 60, 20166, 4, 895, 390, 473, 1677, 75, 1452, 42, 698, 11, 3556, 197, 7457, 6, 1671, 197, 1245, 692, 24, 9687, 185, 4, 60, 1176, 639, 11, 1210, 7, 10, 871, 417, 1939, 78, 206, 301, 20166, 791, 20843, 29509, 118, 371, 4, 534, 6, 8017, 4, 60, 34, 10, 1908, 9, 24, 127, 171, 64, 414, 1592, 128, 7, 908, 10, 67, 10, 98, 5, 4, 68, 35, 3556, 135, 67, 244, 33, 30, 554, 6, 127, 640, 106, 173, 8, 17, 12]),\n",
              "       list([1, 53, 46, 258, 26, 14, 46, 32, 539, 15, 39, 46, 19, 1193, 18, 14, 46, 165, 4692, 18, 86, 19, 7929, 18, 14, 19, 6190, 18, 29, 53, 46, 745, 26, 14, 46, 32, 492, 15, 39, 46, 12, 7499, 18, 14, 46, 165, 4215, 18, 86, 47, 5736, 18, 14, 59, 5210, 18, 17, 12]),\n",
              "       list([1, 53, 46, 312, 26, 14, 74, 134, 26, 39, 46, 5775, 18, 14, 74, 19, 3843, 18, 86, 981, 19, 11, 14, 924, 19, 11, 155, 230, 53, 74, 321, 26, 14, 74, 119, 26, 39, 74, 32, 5328, 18, 14, 74, 32, 3253, 18, 86, 2389, 44, 11, 14, 2012, 61, 11, 17, 12])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiCULh5tAoC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word_index[1]\n",
        "#inv_word_map = {v: k for k, v in word_map.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6W8v9-cF27a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7086cc9-aa2a-451d-c722-574a4e57443b"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18,  3,  3,  9, 19, 19,  4,  4, 11,  3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fLKqFh8DovaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "71153cc6-1fbb-4b3b-dc1d-3cc8d4758bb2"
      },
      "source": [
        "# Demo of encoding\n",
        "\n",
        "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "\n",
        "print(f\"Iran is encoded as {word_index['iran']} in the data\")\n",
        "print(f\"London is encoded as {word_index['london']} in the data\")\n",
        "print(\"Words are encoded as numbers in our dataset.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n",
            "Iran is encoded as 779 in the data\n",
            "London is encoded as 544 in the data\n",
            "Words are encoded as numbers in our dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_QVSlFEAqWJM",
        "colab": {}
      },
      "source": [
        "# Do not change this line. You need the +1 for some reason. \n",
        "max_features = len(word_index.values()) + 1\n",
        "\n",
        "# TODO - your code!\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout\n",
        "from keras.layers import LSTM\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4_sJQsUGAnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0f0a1ecc-d5ad-48bd-a460-47de70b256cf"
      },
      "source": [
        "lstm = Sequential()\n",
        "lstm.add(Embedding(max_features, 128))\n",
        "lstm.add(LSTM(128))\n",
        "lstm.add(Dropout(0.25))\n",
        "lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm.compile(loss='binary_crossentropy',\n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy'])\n",
        "\n",
        "lstm.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, None, 128)         3965440   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 4,097,153\n",
            "Trainable params: 4,097,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4dPqN3VGc29",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "20587d86-017f-4d51-a4ec-cbbbf7dee632"
      },
      "source": [
        "print('Train...')\n",
        "lstm.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test))\n",
        "score, acc = lstm.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/5\n",
            "8982/8982 [==============================] - 34s 4ms/step - loss: -239.3078 - accuracy: 0.0498 - val_loss: -387.0503 - val_accuracy: 0.0396\n",
            "Epoch 2/5\n",
            "8982/8982 [==============================] - 34s 4ms/step - loss: -530.2342 - accuracy: 0.0499 - val_loss: -658.8841 - val_accuracy: 0.0396\n",
            "Epoch 3/5\n",
            "8982/8982 [==============================] - 33s 4ms/step - loss: -801.4213 - accuracy: 0.0499 - val_loss: -925.7846 - val_accuracy: 0.0396\n",
            "Epoch 4/5\n",
            "8982/8982 [==============================] - 33s 4ms/step - loss: -1072.1774 - accuracy: 0.0499 - val_loss: -1192.4731 - val_accuracy: 0.0396\n",
            "Epoch 5/5\n",
            "8982/8982 [==============================] - 33s 4ms/step - loss: -1343.3993 - accuracy: 0.0499 - val_loss: -1457.3386 - val_accuracy: 0.0396\n",
            "2246/2246 [==============================] - 1s 375us/step\n",
            "Test score: -1457.3386065244463\n",
            "Test accuracy: 0.03962600231170654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMXIfk3z-7FR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "2d8d00bd-c85b-4936-89ae-5d513af13fd2"
      },
      "source": [
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
        "print('x_train shape:', X_train.shape)\n",
        "print('x_test shape:', X_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "print('Train...')\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(X_test, y_test))\n",
        "score, acc = model.evaluate(X_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8982 train sequences\n",
            "2246 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (8982, 80)\n",
            "x_test shape: (2246, 80)\n",
            "Build model...\n",
            "Train...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/5\n",
            "8982/8982 [==============================] - 37s 4ms/step - loss: -228.6587 - accuracy: 0.0497 - val_loss: -370.6519 - val_accuracy: 0.0396\n",
            "Epoch 2/5\n",
            "8982/8982 [==============================] - 37s 4ms/step - loss: -510.9044 - accuracy: 0.0499 - val_loss: -638.9728 - val_accuracy: 0.0396\n",
            "Epoch 3/5\n",
            "8982/8982 [==============================] - 39s 4ms/step - loss: -783.0203 - accuracy: 0.0499 - val_loss: -907.9308 - val_accuracy: 0.0396\n",
            "Epoch 4/5\n",
            "8982/8982 [==============================] - 41s 5ms/step - loss: -1055.8138 - accuracy: 0.0499 - val_loss: -1175.9736 - val_accuracy: 0.0396\n",
            "Epoch 5/5\n",
            "8982/8982 [==============================] - 40s 4ms/step - loss: -1325.2728 - accuracy: 0.0499 - val_loss: -1441.0531 - val_accuracy: 0.0396\n",
            "2246/2246 [==============================] - 1s 416us/step\n",
            "Test score: -1441.0530963466301\n",
            "Test accuracy: 0.03962600231170654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "id": "1upEhq2A7brj",
        "colab_type": "text"
      },
      "source": [
        "## Sequence Data Question\n",
        "#### *Describe the `pad_sequences` method used on the training dataset. What does it do? Why do you need it?*\n",
        "\n",
        "Used to ensure all sequences in a list have the same length\n",
        "makes it possible to do batch training \n",
        "\n",
        "\n",
        "\n",
        "## RNNs versus LSTMs\n",
        "#### *What are the primary motivations behind using Long-ShortTerm Memory Cell unit over traditional Recurrent Neural Networks?*\n",
        "\n",
        "LSTMs can access earlier layers whereas RNNs can only remember the last layers\n",
        "This is a very important property when we need the prediction of the neural network to depend on the historical context of inputs, rather than only on the very last input.\n",
        "\n",
        "\n",
        "\n",
        "## RNN / LSTM Use Cases\n",
        "#### *Name and Describe 3 Use Cases of LSTMs or RNNs and why they are suited to that use case*\n",
        "LSTM:\n",
        "For Shakespeare text generation by looking at text and then predicting what the next characters will look like\n",
        "\n",
        "For music generation by learning against musical notes\n",
        "\n",
        "For handwriting generation by learning against hand written letters\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yz0LCZd_O4IG"
      },
      "source": [
        "<a id=\"p2\"></a>\n",
        "## Part 2- CNNs\n",
        "\n",
        "### Find the Frog\n",
        "\n",
        "Time to play \"find the frog!\" Use Keras and [ResNet50v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2) (pre-trained) to detect which of the images with the `frog_images` subdirectory has a frog in it. Note: You will need to upload the images to Colab. \n",
        "\n",
        "<img align=\"left\" src=\"https://d3i6fh83elv35t.cloudfront.net/newshour/app/uploads/2017/03/GettyImages-654745934-1024x687.jpg\" width=400>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGjxIFyb7brl",
        "colab_type": "text"
      },
      "source": [
        "The skimage function below will help you read in all the frog images into memory at once. You should use the preprocessing functions that come with ResnetV2 to help resize the images prior to inference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SY9tEtELPWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2288df5b-a7bf-4930-e1a8-9f18acce5869"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19AVlp6tNEtz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "whIqEWR236Af",
        "colab": {}
      },
      "source": [
        "from skimage.io import imread_collection\n",
        "\n",
        "images = imread_collection('/content/drive/My Drive/Lambda/frog_images/*.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERca6DgyNndq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8786c07a-d19b-4e31-a82d-92b5f866fee3"
      },
      "source": [
        "images[0].shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2137, 1710, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKnnnM8k38sN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "988ea066-5739-490a-def9-42fa9edff6dd"
      },
      "source": [
        "print(type(images))\n",
        "print(type(images[0]), end=\"\\n\\n\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'skimage.io.collection.ImageCollection'>\n",
            "<class 'numpy.ndarray'>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "si5YfNqS50QU"
      },
      "source": [
        "Your goal is to validly run ResNet50v2 on the input images - don't worry about tuning or improving the model. Print out the predictions in any way you see fit. \n",
        "\n",
        "*Hint* - ResNet 50v2 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
        "\n",
        "*Stretch goals:* \n",
        "- Check for other things such as fish.\n",
        "- Print out the image with its predicted label\n",
        "- Wrap everything nicely in well documented fucntions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FaT07ddW3nHz",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, decode_predictions, preprocess_input\n",
        "# TODO - your code!\n",
        "\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "model = ResNet50V2(\n",
        "                   include_top=False,\n",
        "                   weights='imagenet')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz27_u47PpW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def process_img_path(img_path):\n",
        "    return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains_frog(img):\n",
        "    #x = image.img_to_array(img)\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "\n",
        "    features = model.predict(x)\n",
        "\n",
        "    results = decode_predictions(features, top=3)[0]\n",
        "    print(results)\n",
        "    for entry in results:\n",
        "        if entry[1] == 'frog':\n",
        "            return entry[2]\n",
        "    return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNUP4CaxR-et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "48cba17a-1785-44b3-81ac-8758a5134205"
      },
      "source": [
        "img_contains_frog(images[0])"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-5a7fbc1a4c58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_contains_frog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-87-7954e8799708>\u001b[0m in \u001b[0;36mimg_contains_frog\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/resnet_v2.py\u001b[0m in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m   \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mimagenet_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/applications/imagenet_utils.py\u001b[0m in \u001b[0;36mdecode_predictions\u001b[0;34m(preds, top)\u001b[0m\n\u001b[1;32m    138\u001b[0m                      \u001b[0;34m'a batch of predictions '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                      \u001b[0;34m'(i.e. a 2D array of shape (samples, 1000)). '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                      'Found array with shape: ' + str(preds.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mCLASS_INDEX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     fpath = data_utils.get_file(\n",
            "\u001b[0;31mValueError\u001b[0m: `decode_predictions` expects a batch of predictions (i.e. a 2D array of shape (samples, 1000)). Found array with shape: (1, 67, 54, 2048)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HszrN8v8himy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "def process_img_path(img_path):\n",
        "    return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "def img_contains_frog2(img):\n",
        "    #x = image.img_to_array(img)\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    model = ResNet50(weights='imagenet')\n",
        "    features = model.predict(x)\n",
        "    results = decode_predictions(features, top=3)[0]\n",
        "    print(results)\n",
        "    for entry in results:\n",
        "        if entry[1].find('frog') > 0:\n",
        "            return entry[2]\n",
        "    return 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzcd_fD0h0AN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0b716b8c-f007-4d32-8374-fa26de12a8e3"
      },
      "source": [
        "img_contains_frog2(images[0])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_4:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 2137, 1710, 3).\n",
            "[('n01930112', 'nematode', 0.032350022), ('n01496331', 'electric_ray', 0.015001914), ('n03445777', 'golf_ball', 0.014543021)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty8wa9tRirqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1f0b0825-1b70-4795-b997-ec6dd7e81688"
      },
      "source": [
        "img_contains_frog2(images[1])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_5:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 3810, 2856, 3).\n",
            "[('n09472597', 'volcano', 0.12513255), ('n01930112', 'nematode', 0.08108736), ('n03773504', 'missile', 0.048901953)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k64ittbCjBpE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7e3f7de0-f2e7-4362-e102-7b51238bb20c"
      },
      "source": [
        "img_contains_frog2(images[2])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_6:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 3456, 4608, 3).\n",
            "[('n01930112', 'nematode', 0.15511686), ('n04525038', 'velvet', 0.07890502), ('n02219486', 'ant', 0.029604794)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzFouqrYjJcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "53e08377-70a2-4d9e-c983-2fdcbc063849"
      },
      "source": [
        "img_contains_frog2(images[3])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_8:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 2500, 3335, 3).\n",
            "[('n02219486', 'ant', 0.12076042), ('n02233338', 'cockroach', 0.040772017), ('n02259212', 'leafhopper', 0.038669467)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1cjStE-jJlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "278cb9e7-f009-44e4-99bc-4d73a01f7f73"
      },
      "source": [
        "img_contains_frog2(images[4])"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 224, 224, 3) for input Tensor(\"input_9:0\", shape=(None, 224, 224, 3), dtype=float32), but it was called on an input with incompatible shape (None, 2000, 3008, 3).\n",
            "[('n03196217', 'digital_clock', 0.10913357), ('n01833805', 'hummingbird', 0.06624019), ('n01873310', 'platypus', 0.042204827)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmTNaW7ajJr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#attempt at resize\n",
        "from skimage.transform import resize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5czcFFOjJzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = resize(X, (520,256,256,3))\n",
        "#(2137, 1710, 3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnxTtCDYjJ6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image0 = resize(images[0],(224,224,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qY2P84hUmDsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "cfe3ff59-947f-49cc-b255-4bb70280501b"
      },
      "source": [
        "img_contains_frog2(resize(images[1],(224,224,3)))\n",
        "for image in images:\n",
        "  img_contains_frog2(resize(image,(224,224,3)))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('n03729826', 'matchstick', 0.059754852), ('n06359193', 'web_site', 0.055888206), ('n03196217', 'digital_clock', 0.047033764)]\n",
            "[('n06359193', 'web_site', 0.062323112), ('n03196217', 'digital_clock', 0.05368565), ('n01930112', 'nematode', 0.05281234)]\n",
            "[('n03729826', 'matchstick', 0.059754852), ('n06359193', 'web_site', 0.055888206), ('n03196217', 'digital_clock', 0.047033764)]\n",
            "[('n06359193', 'web_site', 0.05612781), ('n03729826', 'matchstick', 0.051401448), ('n03196217', 'digital_clock', 0.0493269)]\n",
            "[('n06359193', 'web_site', 0.06295727), ('n01930112', 'nematode', 0.05260743), ('n03196217', 'digital_clock', 0.04884174)]\n",
            "[('n06359193', 'web_site', 0.06480381), ('n01930112', 'nematode', 0.049625926), ('n03196217', 'digital_clock', 0.045141205)]\n",
            "[('n06359193', 'web_site', 0.06242961), ('n03196217', 'digital_clock', 0.046222176), ('n01930112', 'nematode', 0.04373117)]\n",
            "[('n06359193', 'web_site', 0.055801474), ('n03196217', 'digital_clock', 0.05542864), ('n03729826', 'matchstick', 0.0528601)]\n",
            "[('n03729826', 'matchstick', 0.05113954), ('n06359193', 'web_site', 0.04858198), ('n03196217', 'digital_clock', 0.04785696)]\n",
            "[('n06359193', 'web_site', 0.07434836), ('n01930112', 'nematode', 0.053617623), ('n03196217', 'digital_clock', 0.04667002)]\n",
            "[('n06359193', 'web_site', 0.06386194), ('n01930112', 'nematode', 0.04848813), ('n03196217', 'digital_clock', 0.04335035)]\n",
            "[('n06359193', 'web_site', 0.0648597), ('n01930112', 'nematode', 0.048753846), ('n03196217', 'digital_clock', 0.044015847)]\n",
            "[('n06359193', 'web_site', 0.059280746), ('n03729826', 'matchstick', 0.05076437), ('n01930112', 'nematode', 0.050515868)]\n",
            "[('n03729826', 'matchstick', 0.052103925), ('n04404412', 'television', 0.03866146), ('n03196217', 'digital_clock', 0.03816863)]\n",
            "[('n06359193', 'web_site', 0.062437538), ('n03196217', 'digital_clock', 0.054235496), ('n01930112', 'nematode', 0.04584514)]\n",
            "[('n06359193', 'web_site', 0.066113755), ('n01930112', 'nematode', 0.052815087), ('n03196217', 'digital_clock', 0.04668727)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kfFTfuioW7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XEuhvSu7O5Rf"
      },
      "source": [
        "<a id=\"p3\"></a>\n",
        "## Part 3 - Autoencoders\n",
        "\n",
        "Describe a use case for an autoencoder given that an autoencoder tries to predict its own input. \n",
        "\n",
        "__*Your Answer:*__ \n",
        "An example is learning the representation of MNIST numbers and then recognizing them in handwritten exmaples of these numbers\n",
        "\n",
        "Here's another exmaple \"Collaborative Filtering is a method used by recommender systems to make predictions about the interest of a specific user by collecting taste or preference information from many other users. The technique of Collaborative Filtering has the underlying assumption that if a user A has the same taste or opinion on an issue as the person B, A is more likely to have B’s opinion on a different issue.\n",
        "In this article, you will learn how to predict the ratings a user would give a movie based on this user’s taste and the taste of other users who watched and rated the same and other movies.\" https://towardsdatascience.com/deep-autoencoders-for-collaborative-filtering-6cf8d25bbf1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "626zYgjkO7Vq"
      },
      "source": [
        "<a id=\"p4\"></a>\n",
        "## Part 4 - More..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__lDWfcUO8oo"
      },
      "source": [
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "\n",
        "- What do you consider your strongest area, as a Data Scientist?\n",
        "Just learning to get the hang of it. Very basic skillset\n",
        "- What area of Data Science would you most like to learn more about, and why?\n",
        "CNNs for Perception, I want to use it at work\n",
        "- Where do you think Data Science will be in 5 years?\n",
        "AGI is getting closer. So is self-driving car\n",
        "- What are the threats posed by AI to our society?'\n",
        "mimicking our voices, mimicking videos, mimicking other humans and pretending to be us\n",
        "- How do you think we can counteract those threats? \n",
        "By imposing ethical standards through commitees and government acceptance of best practices across all nations of the world. Having online units to counteract AI that is a threat\n",
        "- Do you think achieving General Artifical Intelligence is ever possible?\n",
        "Yes, in the next 10 years we'll get there\n",
        "A few sentences per answer is fine - only elaborate if time allows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Hoqe3mM_Mtc"
      },
      "source": [
        "## Congratulations! \n",
        "\n",
        "Thank you for your hard work, and congratulations! You've learned a lot, and you should proudly call yourself a Data Scientist.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yYZV0R57brz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"<iframe src=\"https://giphy.com/embed/26xivLqkv86uJzqWk\" width=\"480\" height=\"270\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe><p><a href=\"https://giphy.com/gifs/mumm-champagne-saber-26xivLqkv86uJzqWk\">via GIPHY</a></p>\"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}